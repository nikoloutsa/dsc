Start at Mon Feb  1 16:58:55 EET 2021
CUDA_VISIBLE_DEVICES: 0,1
Running on hosts: gpu[23-38]
Running on 16 nodes.
Running 1 tasks per node
Job id is 1085746
NUM_NODES: 16
srun -w gpu23 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 0 & 
srun -w gpu24 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 1 & 
0: Number of devices per node: 2
srun -w gpu25 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 2 & 
0: Number of devices per node: 2
srun -w gpu26 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 3 & 
0: Number of devices per node: 2
srun -w gpu27 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 4 & 
0: Number of devices per node: 2
0: Number of devices per node: 2
srun -w gpu28 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 5 & 
srun -w gpu29 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 6 & 
srun -w gpu30 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 7 & 
srun -w gpu31 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 8 & 
0: Number of devices per node: 2
0: Number of devices per node: 2
srun -w gpu32 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 9 & 
srun -w gpu33 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 10 & 
0: Number of devices per node: 2
0: Number of devices per node: 2
srun -w gpu34 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 11 & 
srun -w gpu35 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 12 & 
0: Number of devices per node: 2
0: Number of devices per node: 2
srun -w gpu36 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 13 & 
0: Use GPU: 1 for training
srun -w gpu37 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 14 & 
0: Number of devices per node: 2
0: Number of devices per node: 2
0: Number of devices per node: 2
0: Number of devices per node: 2
srun -w gpu38 -N 1 -n 1 -l python -u train.pytorch.imagenet.py --config=configs/pytorch_imagenet_resnet.B64.yaml --dist-url 'tcp://gpu23-ib:5555' --dist-backend 'nccl' --multiprocessing-distributed --world-size 16 --rank 15 & 
0: Number of devices per node: 2
